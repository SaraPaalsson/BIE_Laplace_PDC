\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[normalem]{ulem}

\usepackage{amssymb}

\usepackage{graphicx}
\usepackage[outdir=./]{epstopdf}

\usepackage{cite}

% \renewcommand\topfraction{0.85}
% \renewcommand\bottomfraction{0.85}
% \renewcommand\floatpagefraction{0.85}

\usepackage{float}

\usepackage{tikz}

\def\code#1{\texttt{#1}}


\usepackage{todonotes}


% \usepackage[lined,boxed]{algorithm2e}
\usepackage[linesnumbered,ruled]{algorithm2e}
\SetStartEndCondition{ (}{)}{)}\SetAlgoBlockMarkers{\{}{\}}%
\SetKwProg{Fn}{}{}{}\SetKwFunction{FRecurs}{void FnRecursive}%
\SetKwFor{For}{for}{}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{}{elif}{else}{}%
\SetKwFor{While}{while}{}{}%
\SetKwRepeat{Repeat}{repeat}{until}%
\AlgoDisplayBlockMarkers\SetAlgoNoLine%

\usepackage{geometry}
 \geometry{
 a4paper,
 total={150mm,257mm},
 left=30mm,
 top=40mm,
 bottom=40mm,
 }

\setlength{\parindent}{0ex}
\setlength{\parskip}{1em}

\usepackage{tikz}
\usetikzlibrary{3d,calc}
\usetikzlibrary{arrows, decorations.markings}
\tikzstyle{vecArrow} = [thick, decoration={markings,mark=at position
   1 with {\arrow[semithick]{open triangle 60}}},
   double distance=1.4pt, shorten >= 5.5pt,
   preaction = {decorate},
   postaction = {draw,line width=1.4pt, white,shorten >= 4.5pt}]



\title{Boundary Integral Equations - Project \\ Nearly Singular Integrals and Special Quadrature}
\author{Fredrik Fryklund and Sara P\aa lsson}

\begin{document}

\maketitle

\section*{Introduction}
This project deals with the optimization and parallelization of solving linear equations with boundary integral equation (BIE) methods. To demonstrate the strengths and weaknesses with a BIE method, and how to make the computations for these faster, we use the example problem of solving Laplace's equation in two-dimensions on a starfish domain. Therefore, before going  into details on our existing code, performance analysis and parallelization, we start by overviewing the methods we will use. 

\subsection*{Boundary Integral Equations}
Boundary integral equations provide a way to solve linear equations to a high accuracy. By reformulating the differential equation as an integral equation over the surface of our domain, we reduce the degrees of freedom of the problem. Compared to a standard grid-based method where we discretize the whole domain, for example finite element and finite difference methods, the number of degrees is reduced by one. A simple sketch of the differences between a standard grid-based method and a BIE method can be seen in Fig. \ref{fig:disc}, where we have used squared elements for our standard grid and discretized the BIE with equidistant points along the boundary. The discretization of a starfish domain in two dimensions, which is smooth, is going to be our example domain throughout the report. 
\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=0.45\textwidth]{Graphics/starfish_grid.png}
		\includegraphics[width =0.45\textwidth]{Graphics/starfish_BIE.png}
	\end{center}
	\caption{Typical examples of grid-based discretization (left) and BIE discretization (right) of a starfish domain.}
	\label{fig:disc}
\end{figure}
\\ \\
For completeness, we state Laplaces equation in our domain $\Omega\in\mathbb{C}$, 
\begin{align}
	\begin{split}
		\Delta u = 0, \; z\in\Omega, \\
		u(z) = f(z), \; z\in\delta\Omega,  
	\end{split}
	\label{eq:laplace}
\end{align}
with the analytic function $f(z)=\frac{1}{z-z_1}+\frac{1}{z-z_2} + \frac{1}{z-z_3}$, $z_1,\,z_2,\,z_3\,\notin\Omega$, as boundary condition. As $f$ is analytic, the solution to \eqref{eq:laplace} is known as $u(z)=f(z)$ for all $z\in\Omega$. This provides us with an easy way to check the errors generated by our different quadrature schemes. For a boundary integral method, solving \eqref{eq:laplace} means to first compute the complex density $\mu(z)$ for all $z\in\delta\Omega$ through
\begin{align}
	\frac{1}{2}\mu(z) + \frac{1}{2\pi}\int_{\delta\Omega} \mu(\tau) \Im\left\{ \frac{d\tau}{\tau-z}\right\} = f(z).
	\label{eq:density}
\end{align}
For the points $z = \tau$ on the boundary, we have the limit 
\begin{align*}
\lim_{z\rightarrow\tau} \Im\left\{ \frac{\tau^\prime}{\tau-z}\right\} ) = \Im\left\{ \frac{\tau^{\prime\prime}}{\tau-z} \right\}.
\end{align*}
When we have computed $\mu$, we can compute the solution $u(z)$ at all points $z$ both on the domain and in the boundary, through
\begin{align}
	u(z) = \frac{1}{2\pi} \int_{\delta\Omega} \mu(\tau) \Im\left\{ \frac{d\tau}{\tau-z} \right\}.
\label{eq:ulapl}
\end{align}
There are several advantages with a boundary discretization: the reduction of degrees of freedom gives a smaller system to solve in \eqref{eq:density}, we know the position of the interface explicitly and do not need to interpolate from a domain-based grid, and we can easily track movement of the interface. Other methods for tracking interfaces, such as immersed boundary methods, perform only to low orders of accuracy, whilst a BIE obtain spectral accuracy. The accuracy of a BIE method is decided by the quadrature, and if we use  a high-order quadrature method to evaluate our integrals our method will be of high order. There are of course also drawbacks with this kind of methods. Firstly, the system we need to solve to compute $\mu$ in \eqref{eq:density} is dense (albeit smaller than that of a grid-based scheme due to the decrease in degrees of freedom). However, algorithms such as \texttt{gmres} still converge quickly, as our condition numbers are more or less constant as the resolution is increased, and also small. 
\\ \\
As mentioned previosuly, the integrals we evaluate will contain singularities. These do not pose any problem, as we can use known limits or singularity subtraction to handle such cases. However, as we evaluate our integral in \eqref{eq:ulapl} close to the boundary of our domain, the integral will become so-called ``nearly singular''. This means that whilst the integrand mathematically is smooth and well-defined, numerically it will be difficult to resolve and our errors will increase dramatically. To see this specifically, we look at the integrand
\begin{align}
k(\tau,z) = \Im\left\{ \frac{\tau^\prime}{\tau-z}\right\}.
\label{eq:kernel}
\end{align}
As $z$ comes closer and closer to $\tau$, the denominator is approaching zero, and the integrand will therefore grow rapidly. This will be increasingly difficult to resolve, and simply upsampling the discretization on the boundary $\delta\Omega$ will not be sufficient. For these cases, we need to do something different and will apply a special quadrature.
\\ \\
A short summation of the special quadrature follows. It was originally developed by Helsing and Ojala\footnote{Helsing, J. and Ojala, R., 2008. On the evaluation of layer potentials close to their sources. {\em Journal of Computational Physics}, 227(5), pp.2899-2921.}. When it comes to solving \eqref{eq:ulapl} with special quadrature, we use the fact that the kernel \eqref{eq:kernel} is the same as in the monomials 
\begin{align}
    p_k(z) = \int \frac{\tau^k}{\tau-z}d\tau, \; k=0,\hdots,31.
    \label{eq:pkrec}
\end{align}
For $p_0(z)$ we have the analytical solution 
\begin{align}
    p_0(z) = \log(1-z)-\log(-1-z).
    \label{eq:p0}  
\end{align}
The other $p_k$ can be obtained through a simple recursion. Thus, for a panel $P_j$ we can write
\begin{align}
    \int_{P_j} \mu(\tau) \Im\left\{ \frac{d\tau}{\tau-z} \right\} = \Im\left\{ \sum_{k=0}^{31} c_k \int_{P_j} \tau^k\frac{d\tau}{\tau-z} \right\} = \Im\left\{ \sum_{k=0}^{31} c_kp_k \right\},
    \label{eq:specQ}
\end{align}
where $\mu_j(z) = \sum_{k=0}^{31} c_k\tau^k$ is the polynomial interpolation of $\mu$ restricted to panel $P_j$. 
\\ \\
As a demonstration for how the special quadrature works, we show in Fig. \ref{fig:normquad} how the error in $u$ behaves as we approach the boundary $\delta\Omega$. The number of panels influences the error, but as can be seen upsampling does not completely remove the problems. In comparison, in Fig. \ref{fig:specquad} we see the error after applying the special quadrature. Also for 35 panels, the errors are negligble. 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/filled_error_panels35.png}
        \includegraphics[width =0.49\textwidth]{Graphics/filled_error_panels70.png}
    \end{center}
    \caption{Example of error in domain when solving Laplace's equation, using only normal quadrature, using 35 panels (left) and 70 panels (right).}
    \label{fig:normquad}
\end{figure}
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/fillederror_SQ_panels35.png}
        \end{center}
    \caption{Error after application of special quadrature, for 35 panels.}
    \label{fig:specquad}
\end{figure}
If we zoom in and compare the level curves for the error in the black box, we see that the special quadrature removes all errors up to $\sim 10^{-15}$, see Fig. \ref{fig:levelcurves}. Here the black curves correpond to different magnitudes of error. As is observable in the right most image, the special quadrature effectively reduces the error.
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/contour_panels35.png}
        \includegraphics[width =0.49\textwidth]{Graphics/contour_SQ_panels35.png}
    \end{center}
    \caption{Error curves, for normal quadrature, $\log_{10} e(z) = \{ -15,-12,-9,-6, -3 \}$ (left) and special quadrature (only $\log_{10} e(z) = -15$ visible) (right).}
    \label{fig:levelcurves}
\end{figure}
\FloatBarrier

\section*{The Algorithm}
The algorithm to compute the boundary integral solution is divided up into several parts. The three major ones are to compute the complex density $\mu$ through \eqref{eq:density}, compute the solution $u$ everywhere in the domain through \eqref{eq:ulapl} and finally to compute corrections to $u$ using special quadrature in those cases it is needed. When discretizing our boundary $\delta\Omega$, we divide it into panels on which we put 16 Gauss-Legendre nodes each. The domain is discretized radially and angularly, going from the middle outwords towards $\delta\Omega$ anti-clockwise. Here, we describe each algorithm briefly. The algorithms can be found in Appendix \ref{sec:alg}. 
\\ \\
First, in Alg. \ref{alg:solveDensity} we solve \eqref{eq:density} for the complex density $\mu$. This is done by first filling the system matrix $A$, through two \textit{for}-loops. We then compute $\mu$ through \code{gmres}. Our matrix $A$ is dense and real-valued, but not symmetric. The major parts of the algorithm are thus two for-loops of the \textbf{boundary} discretization points, and a call to \texttt{gmres}.
\\ \\
The next step is to compute the solution in the domain through \eqref{eq:ulapl}. We do this by integrating our $\mu$ over the boundary $\delta\Omega$, using the same discretization as in Alg. \ref{alg:solveDensity}. This becomes one for-loop over the \textbf{domain} discretization points and one over the \textbf{boundary} discretization points (typically fewer than those on the boundary), see Alg. \ref{alg:compU}.
\\ \\
The final major step of the algorithm is to apply the special quadrature  to reduce the errors in $u$ close to the boundary. This algorithm is briefly described in Alg. \ref{alg:specQ}. It consists of a for-loop over all domain points $\tau_i$, and then a for-loop over all panels $P_j$ of the domain discretization. For each panel and point pair, the distance between them is computed. If the distance is less than a set threshold, further investigations are needed. We will compute an analytical and numerical value of $p_0$ \eqref{eq:p0}, which contains the same kernel as our solution integral. If these two values of $p_0$ differ by less than a constant, our integral is deemed well resolved. If not, we upsample to 32 points Gauss-Legendre quadrature on the panel, and compare $p_0$ again. Now, if the upsampled integral gives a good value of $p_0$ we assume that also our integral for $u_i$ will be well resolved for point $\tau_i$ on panel $P_j$. We therefore compute a new value for $u_i$ on $P_j$ only, and add it to $u_i$. If upsampling is not enough, we apply our special quadrature. We then need to compute all $p_k$ through a recursion, \eqref{eq:pkrec}, and then find the coefficient to interpolate this to a polynomial of degree 15. This is done through solving a Vandermonde system. We then compute our new value of $u_i$ on panel $P_j$ with the specal quadrature \eqref{eq:specQ}. 
\FloatBarrier

\section*{Performance of Serial Code}
We analyze our code using CrayPAT as well as a function to time the separate functions individually without CrayPAT (convenient when making several runs). We test both the dependency on the number of panels $N_{pan}$ as well as the number of discretization points $N_{dom}$.

\subsection*{Changing Domain Discretization}
Firstly, we investigate how changing the number of domain points, $N_{dom}$, influences our codes performance. In these tests, we have set $N_{pan}=20$ which gives us an error of $\approx 10^{-8}$. For some different $N_{dom}$ ranging from $25$ to $13$ million we see the timings in Fig. \ref{fig:serial1}. For small $N_{dom}$, the total computation time is dominated by the time it takes to solve the linear system when computing the complex density $\mu$. For this example that cost will be constant as we keep $N_{pan}$ constant, and $\mu$ is independent of the domain resolution. As $N_{dom}$ grows, the cost will be dominated by the computation of $u$ with standard quadrature and the special quadrature. They both show the same cost, which is natural as the main feature of both algorithms is a for-loop through all points. We can also see this as they show complexity $O(N)$, which a for-loop would. 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/timing_serial_npanels20.png}
    \end{center}
    \caption{Timings serial code for varying $N_{dom}$. Fixed $N_{pan}=20$.}
    \label{fig:serial1}
\end{figure}

\subsubsection*{Performance Analysis}
Next, we investigate the code with CrayPAT as we change $N_{dom}$. We start by studying a problem were we are still using a relative small amount of points in our domain, i.e. were \texttt{solveDensity}, Alg. \ref{alg:solveDensity}, dominates the cost. This problem is run for $N_{dom} = 1600$, $N_{pan}=20$. When studying the performance through CrayPAT, we see that the call to \texttt{gmres} takes most time, see Fig. \ref{fig:batch4_serial_pie}. We set that the functions being called the most times, such as \texttt{IPMultR} and \texttt{tau} e.g., are very quick, whereas the three major algorithms pin-pointed earlier indeed take the most time. We see in the right pie chart, that it is the \texttt{gmres}-call that takes up the most time. The function \texttt{IPMultR} upsamples from $16$ to $32$ Gauss-Legendre points. It is called upon by \code{specialquadlapl} when a point is considered to be to close to the boundary. 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/craypat_serial_batch4_calls_pie.png}
        \includegraphics[width=0.49\textwidth]{Graphics/craypat_serial_batch4_time_pie.png}
    \end{center}
    \caption{Number of function calls and times for serial code $N_{dom} = 1600$, $N_{pan} = 20$.}
    \label{fig:batch4_serial_pie}
\end{figure}
Furthermore, if we investigate (USER) cache misses, we see that our code has a quite bad hit/miss ratio, see Tab. \ref{tab:batch4_serial_caches}. Our total D1 cache hit/miss ratio, however, seem to be decreased by the function call to \texttt{call\_gsl\_gmres}, which is the \texttt{gmres} solver in the GSL-package. Other bad results worth noticing and investigating further, seem to be the poor D2 hit/miss ratio by \texttt{computeSolution}. 
\begin{table}[ht]
\begin{center}
    \begin{tabular}{ c | c | c | c | c }
        Function & D1 hit/miss (\%/\%) & D2 hit/miss & D1+D2 hit/miss & Time (\%) \\ \hline
        Total & $47.9/52.1$ & $28.3/71.7$ & $62.7/37.3$ & $100$ \\ 
        call\_gsl\_gmres & $23.5/76.5$ & $31.4/68.6$ & $47.5/52.5$ & $70.9$ \\
        \code{specialquadlapl} & $97,8/2.2$ & $100/0$ & $100/0$ & $11.4$ \\
        {computeSolution} & $100/0$ & $0/100$ & $38.1/61.9$ & $8.9$ \\
        \code{gl16} & $98.5/1.5$ & $14.2/85.8$ & $79.82/9.98$ & $2.7$ \\
        \code{vandernewton} & $99.8/0.2$ & $100/0$ & $100/0$ & $2.6$ \\
        \code{solveDensity} & $99.8/0.2$ & $51.3/48.7$ & $99.9/0.1$ & $2.5$
    \end{tabular}
    \caption{D1 and D2 cache hit and miss ratios for serial code, $N_{dom} = 1600$. Data generated by CrayPAT.}
    \label{tab:batch4_serial_caches}
\end{center}
\end{table}  \\
Now, we study the timings and cache missed for a problem with larger $N_{dom}$, i.e. when those computations dominates over the panel computations. We set $N_{dom} = 3276800$ and keep, as before, $N_{pan}=20$. In Fig. \ref{fig:batch10_serial_pie} (left) we see that the number of function calls is dominated, still, by \texttt{IPmultR} and \texttt{vandernewton}. The functions \texttt{tau, taup, taupp} no longer influence the computations as they are called only once in the beginning of the simulation, to set up the boundary domain. Regarding the time consumption, it is now clear (right figure) that the call to \texttt{gmres} is no longer of any significance. Now the computation is dominated instead by \texttt{specquadlapl} and \texttt{computeSolution}.
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/craypat_serial_batch10_calls_pie.png}
        \includegraphics[width=0.49\textwidth]{Graphics/craypat_serial_batch10_time_pie.png}
    \end{center}
    \caption{Number of function calls and times for serial code $N_{dom} = 1600$, $N_{pan} = 20$.}
    \label{fig:batch10_serial_pie}
\end{figure} \\ \\
In comparison to the example with smaller $N_{dom}$, our cache utilization has now significantly approved among the costly functions. For all functions which make up the majority of the execution time all have D1 cache hit and miss ratios of $>99\%$. Also the D1+D2 cache hit,miss ratios are all $>99\%$. However, the D2 cache hit$/$miss ratios are worse, ranging from $55\%$ to $80.1\%$. 
\begin{table}[ht]
\begin{center}
    \begin{tabular}{ c | c | c | c | c }
        Function & D1 hit/miss (\%/\%) & D2 hit/miss & D1+D2 hit/miss & Time (\%) \\ \hline
        Total & $99.3/0.7$ & $67.8/32.2$ & $99.8/0.2$ & $100$ \\ 
        specialquadlapl & $99.1/0.9$ & $72.1/27.9$ & $99.7/0.3$ & $47.6$ \\
        computeSolution & $100/0$ & $80.1/19.9$ & $100/0$ & $38.5$ \\
        vandernewton & $99.4/0.6$ & $55/45$ & $99.7/0.3$ & $10.8$ \\
        IPmultR & $99.3/0.7$ & $68.8/31.2$ & $99.8/0.2$ & $1.6$
    \end{tabular}
    \caption{D1 and D2 cache hit and miss ratios for serial code, $N_{dom} = 3276800$. Data generated by CrayPAT.}
    \label{tab:batch10_serial_caches}
\end{center}
\end{table}  

% \subsection*{Change boundary discretization}
% Contrary to what we did previously, we will now see how the performance changes as we change $N_{pan}$ and keep $N_{dom}$ fixed. As a reference, we have selected a suitable $N_{dom} = 409 600$.
% \todo[inline]{Do we need this?}
% \todo[inline]{Define our testcase that we will test for different $N$.}
\FloatBarrier

\section*{Speeding Up Our Code}
From the tests deducted on our serial code, we conclude that in the simulation cases we are interested in studying, i.e. for problems dominated by $N_{dom}$ over $N_{pan}$, it is most important to work with \texttt{specquadlapl} and \texttt{computeSolution} to achieve a speed-up. 
\\ \\
Regarding Table \ref{tab:batch4_serial_caches} and \ref{tab:batch10_serial_caches}, we see that the functions which take the majority of the execution code show good cache hit/miss ratios. In that regard, it is therefore not necessary to change the code for better cache perfomance. 

\subsection*{OpenMP}
When parallelizing with OpenMP, we focus on \texttt{computeSolution} and \texttt{specialquadlapl} as they take the most time for large problems. Since both of them are based on for-loops stepping through all the domain points, which are treated independently, it is straightforward to distribute the points among the threads. We create new thread pools inside the two functions, instead of creating a common thread pool during initialization. This adds overhead in the creation, but is very simple to implement. To achieve good load balancing, we initially distribute our domain discretization points radially. This is because points close to the boundary will be significantly more expensive than those far away. Those close to the boundary will trigger \texttt{specialquadlapl}. Moreover, to avoid load imbalancing, we divide the for-loops in this function with the \texttt{guided} schedule. This means that the threads receiving chunks of ``easy'' points will finish sooner and obtain further blocks. As a result we have $0\%$ imbalancing for our OpenMP-code. 
\\ \\
\subsubsection*{Strong Scaling}
Firstly, we investigate the strong scaling of our algorithm with OpenMP-parallelization. This is done by keeping the number of discretization points in the domain fixed (and quite large) at $15\cdot10^{6}$, and set $N_{panels}=20$. In Fig. \ref{fig:omp_strongtime}, we see how the computation times scale with the number of OpenMP-threads. Except for a very small deviation for a large number of threads, the time scales linearly. This is no surprise due to the embarassingly parallel nature of the problem (however, still a good pedagogical basis on which to learn the concepts of parallel programming). 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/omp_fixed_domain_size_loglog.png}
    \end{center}
    \caption{Strong scaling results for OpenMP-parallelized code, $N_{dom}=15\cdot 10^6$.}
    \label{fig:omp_strongtime}
\end{figure}
\\ \\
For completeness, we also study the cache hit/miss ratios of our parallelized problem in Table \ref{tab:openmp_caches}. As for the larger problem with the serial code, see Table \ref{tab:batch10_serial_caches}, the ratios are good for all our functions. As before, \texttt{IPMultR} dominates the number of function calls ($85\%$), but is a small easy function called by \texttt{specialquadlapl} and does not dominate the function time ($6\%$), see Fig. \ref{fig:omp_strong_pie}. 
\begin{table}[ht]
\begin{center}
    \begin{tabular}{ c | c | c | c | c }
        Function & D1 hit/miss (\%/\%) & D2 hit/miss & D1+D2 hit/miss & Time (\%) \\ \hline
        Total & $98.6/1.4$ & $72.8/27.2$ & $99.6/0.4$ & $100$ \\ 
        \code{computeSolution} & $99.3/0.7$ & $75.0/25.0$ & $99.8/0.2$ & $33.7$ \\
        \code{specialquadlapl} & $98.6/1.4$ & $95.5/4.5$ & $99.9/0.1$ & $29.3$ \\
        \code{create\_grid} & $100.0/0$ & $92.5/7.5$ & $100.0/0$ & $10.4$ \\
        \code{vandernewton} & $99.1/0.9$ & $96.2/3.8$ & $100.0/0$ & $6.4$ \\
        \code{IPmultR} & $98.5/1.5$ & $93.3/6.7$ & $99.9/0.1$ & $6.0$
    \end{tabular}
    \caption{D1 and D2 cache hit and miss ratios for OpenMP-parallelized code, $N_{dom} = 15\cdot10^6$. Number of threads $32$. Data generated by CrayPAT.}
    \label{tab:openmp_caches}
\end{center}
\end{table} 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/omp_strong_pie_calls.png}
        \includegraphics[width=0.49\textwidth]{Graphics/omp_strong_pie_time.png}
    \end{center}
    \caption{Number of function calls and times for OpenMP-parallelized code $N_{dom} = 15\cdot10^6$, $N_{pan} = 20$.}
    \label{fig:omp_strong_pie}
\end{figure}

\subsubsection*{Weak Scaling}
To investigate the weak scaling of the problem, we regard a fixed problem size per thread, i.e. $4\cdot10^5$ points/thread (since $32\cdot4\cdot10^5 \approx 15\cdot 10^{6}$). When increasing the number of threads, we thus increase also the problem size. Optimally, for a problem like this the computational time should stay constant. As can be seen in Fig. \ref{fig:omp_weaktime}, we experience a slight increase in computational time when increasing the number of threads. For the two functions, the increase can probably be explained by the overhead of creating more threads and divide a larger number of discretization points between them. This increase could probably be minimized by working with one pool of threads for both functions.  The larger increase in total time comes from the functions which have not been parallelized, e.g. creating the domain. If this was perfect weak scaling in a theoretical sense, the number of threads should not influence computational time.
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/omp_fixed_pntsPerThread_size_loglog.png}
    \end{center}
    \caption{Weak scaling results for OpenMP-parallelized code, $N_{dom}/thread=4\cdot 10^5$.}
    \label{fig:omp_weaktime}
\end{figure}

\FloatBarrier


\subsection*{MPI}
We parallelize the code with MPI, using a master-slave concept. The master initializes the grid, and computes the density through \textit{solveDensity}. All information needed for \texttt{computeSolution}, \texttt{specialquadlapl} and \texttt{computeError} is then scattered among the threads. When the threads finish their computations the results are gathered by the master. Overall this creates a larger overhead than previously with OpenMP, where we could use a suitable scheduling to avoid this problem, as some processes will finish much sooner than others if their point-distribution is favourable. We have tried to initialize our domain discretization in such a way to minimize this problem, but \texttt{specialquadlapl} still experiences a load imbalancing of about $70\%$. In contrast, \texttt{computeSolution} has a load imbalance of $\approx0.01\%$. 

\subsubsection*{Strong Scaling}
As for the OpenMP-parallelization, we regard a problem with domain discretization size $15\cdot10^6$ and $N_{panels}=20$. In Fig. \ref{fig:mpi_strongtime} we see how the execution time changes when we increase the number of MPI tasks. Note that the actual computation times for both \code{computeSolution} and \code{specialquadlapl} decrease linearly with the number of tasks, but the increased computation time for \code{main} makes the total time speed-up much less favourable. This is due to load imbalancing, i.e. some tasks finishing slower than others and the master waiting for them. As we can see in Fig. \ref{fig:mpi_loadimb}, for the strong scaling case, when we increase the number of MPI tasks, the load imbalance time for \code{specialquadlapl} decreases. This means that the more tasks we have, for a constant number of domain points, we spread the worst-case-scenario points between more threads. 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/craypat_mpi_strongscaling.png}
    \end{center}
    \caption{Strong scaling results for MPI-parallelized code, $N_{dom}=15\cdot 10^6$.}
    \label{fig:mpi_strongtime}
\end{figure}
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/mpi_loadimb.png}
    \end{center}
    \caption{Load imbalance time for \code{specialquadlapl}, for MPI-parallelized code, strong and weak scaling.}
    \label{fig:mpi_loadimb}
\end{figure}
\\ \\
For completeness, we again plot the cache miss/hit ratios in Table \ref{tab:mpi_caches}, which we still show very good hit/ratio percentages. As previously, the function calls and time is dominated by \code{IPmultR}, see Fig. \ref{fig:mpi_pie}.

\begin{table}[ht]
\begin{center}
    \begin{tabular}{ c | c | c | c | c }
        Function & D1 hit/miss (\%/\%) & D2 hit/miss & D1+D2 hit/miss & Time (\%) \\ \hline
        Total & $99.1/0.9$ & $70.7/29.3$ & $99.7/0.3$ & $100$ \\ 
        \code{main} & $100/0$ & $75.0/25.0$ & $100.0/0$ & $77.3$ \\
        \code{specialquadlapl} & $99.2/0.8$ & $71.9/28.1$ & $99.8/0.2$ & $9.5$ \\
        \code{computeSolution} & $100/0$ & $98.5/1.5$ & $100/0$ & $9.3$ \\
        \code{vandernewton} & $99/1.0$ & $70.2/29.8$ & $99.7/0.3$ & $2.1$ \\
        \code{IPmultR} & $99.1/0.9$ & $69.4/30.6$ & $99.7/0.3$ & $1.5$
    \end{tabular}
    \caption{D1 and D2 cache hit and miss ratios for MPI-parallelized code, $N_{dom} = 15\cdot10^6$, for $32$ MPI-processes. Data generated by CrayPAT.}
    \label{tab:mpi_caches}
\end{center}
\end{table} 
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{Graphics/craypat_mpi_threads32_calls_pie.png}
        \includegraphics[width=0.49\textwidth]{Graphics/craypat_mpi_threads32_time_pie.png}
    \end{center}
    \caption{Number of function calls and times for MPI-parallelized code $N_{dom} = 15\cdot10^6$, $N_{pan} = 20$.}
    \label{fig:mpi_pie}
\end{figure}


\subsubsection*{Weak Scaling}
When investigating the weak scaling of the problem, Fig. \ref{fig:mpi_weaktime}, we see that \code{computeSolution} scales well and remains almost constant. We also see that \code{specialquadlapl} scales well. However, as the number of tasks increases our \code{main} gets more expensive, and thus also the total time. This is because for more processes the amount of communication needed increases, and also the waiting time. The consequence is bad scaling, in a weak sense,for the whole algorithm. As previously, we have used $4\cdot 10^5$ points per MPI-task, and $N_{panels}=20$.
\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/craypat_mpi_weakscaling.png}
    \end{center}
    \caption{Weak scaling results for MPI-parallelized code, $N_{dom}/thread=4\cdot 10^5$.}
    \label{fig:mpi_weaktime}
\end{figure}


\FloatBarrier

\subsection*{Hybrid Method}
When parallelizing our code with both MPI and OpenMP, we divide the computational domain for the MPI tasks and then assign a certain number of threads for each when computing \code{computeSolution} and \code{specialquadlapl}. The total number of $32$ threads is spread over a varying number of MPI tasks. As previously, we set $N_{dom}=15\cdot10^6$. 
\\ \\
We observe the same behaviour as for MPI in Fig. \ref{fig:hybrid}, i.e. the more MPI tasks we use the more \code{main} dominates our execution time. \code{computeSolution} and \code{specialquadlapl} is largely unaffected.  



\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Graphics/hybrid_plot.png}
    \end{center}
    \caption{Timings for hybrid code, $N_{dom}=15\cdot 10^6$.}
    \label{fig:hybrid}
\end{figure}
\FloatBarrier



\section*{Conclusions}
MPI might not be the best solution for our problem. This because it creates unnecessary overhead, as all tasks need their own copy of all initial data. This is a substantional amount of data that need to be passed. Additionally, the communication between master and slaves takes time. Using a hybrid version does not make the situation much better. Therefore, OpenMP is preferable to our problem. It seems to scale very well, both in the strong and weak sense, actually as well as theory allows.

\newpage
\appendix
\section{Algorithms}
\label{sec:alg}
\begin{algorithm}[ht]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \underline{Function solveDensity} $(z,zp,zpp,W, RHS)$\;
    \Input{Interface discretization points and derivatives: $z$, $zp$, $zpp$. Quadrature weights: $W$. Right hand side: $RHS$.}
    \Output{$\mu$}
    \For{go through all boundary points $i$} {
        \For{go through all boundary points $j$} {
            Fill dense matrix $A$: \\
                Elements $A_{ij} = \frac{1}{2\pi} W_j \Im\left\{ \frac{zp_j}{z_j - z_i} \right\}$. \\
                Elements $A_{ii} = \frac{1}{2} + \frac{1}{2\pi}W_i \Im\left\{ \frac{zpp_i}{2zp_i} \right\}$.
        }
    }
    Solve $A\mu=RHS$ with gmres.
    \caption{Computing complex density $\mu(z)$, for all $z\in\delta\Omega$.}
    \label{alg:solveDensity}
\end{algorithm}
\begin{algorithm}[ht]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \underline{Function computeSolution} $(z,zp,\tau,W,\mu)$\;
    \Input{Interface discretization points and derivatives: $z$, $zp$. Domain discretization points $\tau$. Quadrature weights: $W$. Complex density $\mu$.}
    \Output{$u$}
    \For{go through all domain points $\tau_i$} {
        $u_i = 0$; \\
        \For{go through all boundary points $z_j$} {
            Compute velocity $u_i$: 
            $u_i = u_i + W_j\mu_j\Im\left\{ \frac{zp_j}{z_j-\tau_i} \right\}$;
        }
    }
    \caption{Computing solution to Laplace's equation $u$, for all $\tau\in\Omega$.}
    \label{alg:compU}
\end{algorithm}
\begin{algorithm}[ht]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \underline{Function specialQuadrature} $(\tau,W, P_j, \mu, u)$\;
    \Input{Interface discretization points and derivatives: $z$, $zp$. Domain discretization points $\tau$. Quadrature weights: $W$. Complex density $\mu$. Solution $u$ computed with standard quadrature.}
    \Output{$u$}
    \For{go through all domain points $\tau_i$} {
        \For{go through all panels $P_j$} {
            Compute distance between $P_j$ and $\tau_i$, $d_{ij}$
            \If{$d_{ij} < TOL$} {
                Investigate this point-panel pair further: \\
                \begin{itemize}
                    \item Compute analytical solution to $p_0$.
                    \item Go through all panel points (16) and compute numerical approximation to $p_0$, $\hat{p}_0$.
                \end{itemize}
                \If{ $|p_0 - \hat{p}_0| > TOL$} {
                    Normal quadrature not sufficient to resolve this domain point.
                    Upsample to $32$-point Gauss-Legendre panel on $P_j$, $\tilde{z}_i$, and comupte $\hat{p}_0$ again. 
                    \If{ $|p_0 - \hat{p}_0| > TOL$} {
                        Upsample sufficient for resolution. Compute panel contribution anew by going throuhg points $\tilde{z}_i$. Remove panel contribution from 16-points G.-L. from $u$ and add new contribution from 32-points.
                    }
                    \Else {
                        Upsampling not sufficient. Apply special quadrature by computing all $p_k$ and interpolating, see REF. Then remove panel contribuiton from 16-points G.-L. from $u$ and add new solution. 
                    }
                }
                \Else {
                    Normal quadrature is sufficient. No recomputation needed.
                }
            }
        }

    }
    \caption{Computing solution to Laplace's equation $u$ with special quadrature, for all $\tau\in\Omega$.}
    \label{alg:specQ}
\end{algorithm}


\end{document}

